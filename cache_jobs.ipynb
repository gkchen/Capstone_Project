{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from random import sample\n",
    "import dill\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and cache .zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _no_location(df):\n",
    "    truth = np.array(df['region'].isna().tolist() and df['locality'].isna().tolist())\n",
    "    idx = df[truth].index\n",
    "    df.drop(idx, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _abrv_states(df):\n",
    "    df['region'] = df['region'].str.upper().replace(states_dict)\n",
    "    return None\n",
    "\n",
    "def _in_usa(df):\n",
    "    truth = df[['region']].isin(states_dict.values())['region']\n",
    "    idx = df[~truth].index\n",
    "    df.drop(idx, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _has_title(df):\n",
    "    df.dropna(subset=['title'], inplace=True)\n",
    "    return None\n",
    "\n",
    "def _combine_dates(df):\n",
    "    df['posted_date'].fillna(df['date_added'], inplace=True)\n",
    "    df.drop('date_added', axis=1, inplace=True)\n",
    "    df.rename(columns={'posted_date': 'date'}, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _has_dates(df, columns):\n",
    "    df.dropna(subset=columns, how='all', inplace=True)\n",
    "    return None\n",
    "\n",
    "def _date_parser(s):\n",
    "    output = pd.to_datetime(s, format='%Y-%m-%d', errors='coerce')\n",
    "    return output\n",
    "\n",
    "def _clean_and_save_chunk(file, num=0, **kwargs):\n",
    "    for chunk in pd.read_csv(file, **kwargs):\n",
    "        _has_title(chunk)\n",
    "        _has_dates(chunk, columns=date_cols)\n",
    "        _abrv_states(chunk)\n",
    "        _in_usa(chunk)\n",
    "        chunk.reset_index(drop=True).to_feather('raw_cache/data_{}.feather'.format(num))\n",
    "        num += 1\n",
    "    return num\n",
    "\n",
    "def cache_files(files, num=0, **kwargs):\n",
    "    for file in tqdm(files, desc='zip files'):\n",
    "        num = _clean_and_save_chunk(file, num=num, **kwargs)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_feather('other_data/census.feather')\n",
    "states = pd.read_feather('other_data/us_states.feather')\n",
    "states_dict = states.set_index('STATE').to_dict()['Abrv']\n",
    "\n",
    "zip_columns = ['title', 'brand', 'category', 'locality', 'region', 'date_added', 'posted_date']\n",
    "date_cols = ['date_added', 'posted_date']\n",
    "\n",
    "start = pd.datetime(2017, 12, 1)\n",
    "end = pd.datetime(2018, 7, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac11149672e420aac28d52555acb444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='zip files', max=7, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder = 'raw_zips'\n",
    "files = [os.path.join(folder, file) for file in os.listdir(folder)]\n",
    "cache_files(files, usecols=zip_columns, chunksize=1e7, compression='infer', dtype=str, parse_dates=date_cols, date_parser=_date_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group data\n",
    "\n",
    "Group by day, week, state, and city over time and cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_df(file, **kwargs):\n",
    "    df = pd.read_feather(file, **kwargs)\n",
    "    _within_range(df)\n",
    "    df.dropna(subset=['posted_date'], inplace=True)\n",
    "#     _add_day_of_week(df)\n",
    "    return df\n",
    "\n",
    "def _within_range(df):\n",
    "    start = pd.datetime(2017, 1, 1)\n",
    "    end = pd.datetime(2018, 7, 1)\n",
    "    truth = ~df['posted_date'].isin(pd.date_range(start, end))\n",
    "    df.drop(df[truth].index, inplace=True)\n",
    "\n",
    "def _add_day_of_week(df):\n",
    "    days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday']\n",
    "    df['day_of_week'] = pd.Categorical(df['posted_date'].dt.day_name(), categories=days, ordered=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3e7d04408b4adc97fa9c28aa9df806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='feather_files', max=41, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "job_counts = None\n",
    "folder = 'raw_cache'\n",
    "files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.feather')]\n",
    "\n",
    "for file in tqdm(files, desc='feather_files'):\n",
    "    df = _get_df(file, columns=['locality', 'region', 'posted_date'])\n",
    "    df['posts'] = 1\n",
    "    grouped = df.groupby(['locality', 'region', 'posted_date']).sum()\n",
    "    if job_counts is None:\n",
    "        job_counts = pd.Series()\n",
    "        job_counts.name = 'posts'\n",
    "        job_counts = job_counts.add(grouped['posts'], level='locality', fill_value=0)\n",
    "    else:\n",
    "        job_counts = job_counts.add(grouped['posts'], fill_value=0)\n",
    "        \n",
    "job_counts = job_counts.reset_index()\n",
    "job_counts['posts'] = job_counts['posts'].astype(int)\n",
    "job_counts.to_feather('grouped/job_counts.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "I want to apply SVD onto a sparse matrix of counted terms from `CountVectorizer` to get the principle axes, but `TruncatedSVD` from `sklearn` is too memory intensive and takes a long time. A solution is to possibly find an on-line algorithm that is something like gradient descent for SVD. I found a couple of resources to go through for this:\n",
    "\n",
    "- [stack exchange](https://stats.stackexchange.com/questions/177007/updating-svd-decomposition-after-adding-one-new-row-to-the-matrix)\n",
    "- [gensim](https://pypi.org/project/gensim/)\n",
    "- [surprise](http://surpriselib.com/)\n",
    "- [sparsesvd](https://pypi.org/project/sparsesvd/)\n",
    "\n",
    "I'm leaning towards `gensim` at the moment, there are good resources for it and it seems like it is widely used for this expressed purpose, in particular, the `Latent Semantic Indexing` transformation. [Data Camp](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python) has a resource outlining this exact procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk import download\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary, doc_term_matrix\n",
    "\n",
    "def create_gensim_lsa_model(doc_clean, number_of_topics):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "#     print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = pd.read_feather('other_data/ignore.feather')\n",
    "ignore = ignore.set_index('regex').to_dict()['sub']\n",
    "\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "class TextPreProcess(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    Input  : document list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ignore):\n",
    "        self.en_stop = set(stopwords.words('english')) # English stop words list\n",
    "        self.tokenizer = RegexpTokenizer(r'[a-z]+&?[a-z]+')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.replace = ignore\n",
    "    \n",
    "    def _process(self, text):\n",
    "        raw = text.lower()\n",
    "        for key, val in self.replace.items():\n",
    "            raw = re.sub(key, val, raw)\n",
    "        tokens = self.tokenizer.tokenize(raw)\n",
    "        stopped_tokens = [i for i in tokens if not i in self.en_stop]\n",
    "        lemma_tokens = [self.lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "        output = ' '.join(lemma_tokens)\n",
    "        return output\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        output = X.apply(self._process)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and cache titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7a8bc38e3b43c69b4188376aa945b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='clean titles', max=41, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder = 'raw_cache'\n",
    "files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.feather')]\n",
    "\n",
    "tpp = TextPreProcess(ignore)\n",
    "\n",
    "i = 0\n",
    "for file in tqdm(files, desc='clean titles'):\n",
    "    df = _get_df(file, columns=['title', 'region', 'posted_date'])\n",
    "    df['title'] = tpp.fit_transform(df['title'])\n",
    "    df.reset_index(drop=True).to_feather('cleaned_titles/titles_{}.feather'.format(i))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4cf05967c04f8ba46f7576fdb36db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='produce dictionary', max=41, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garychen/anaconda3/envs/TDI/lib/python3.7/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    }
   ],
   "source": [
    "def get_dictionary(files)\n",
    "    dictionary = corpora.Dictionary(prune_at=5000)\n",
    "    for file in tqdm(files, desc='produce dictionary'):\n",
    "        df = pd.read_feather(file, columns=['title'])\n",
    "        text = df['title'].str.split()\n",
    "        dictionary.add_documents(text, prune_at=5000)\n",
    "\n",
    "    dictionary.filter_tokens(bad_ids=['nd', 'rd', 'st'])\n",
    "    dictionary.compactify()\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'title_cache'\n",
    "files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.feather')]\n",
    "\n",
    "dictionary = get_dictionary(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save_as_text('transforms/dictionary.bin', sort_by_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5422"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
