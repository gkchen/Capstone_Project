{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "import dill\n",
    "import re\n",
    "import os\n",
    "import matplotlib\n",
    "\n",
    "from sklearn import base\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and cache .zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_feather('other_data/census.feather')\n",
    "states = pd.read_feather('other_data/us_states.feather')\n",
    "states_dict = states.set_index('STATE').to_dict()['Abrv']\n",
    "\n",
    "zip_columns = ['title', 'brand', 'category', 'locality', 'region', 'date_added', 'posted_date']\n",
    "date_cols = ['date_added', 'posted_date']\n",
    "\n",
    "start = pd.datetime(2017, 12, 1)\n",
    "end = pd.datetime(2018, 7, 21)\n",
    "\n",
    "def _no_location(df):\n",
    "    truth = np.array(df['region'].isna().tolist() and df['locality'].isna().tolist())\n",
    "    idx = df[truth].index\n",
    "    df.drop(idx, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _abrv_states(df):\n",
    "    df['region'] = df['region'].str.upper().replace(states_dict)\n",
    "    return None\n",
    "\n",
    "def _in_usa(df):\n",
    "    truth = df[['region']].isin(states_dict.values())['region']\n",
    "    idx = df[~truth].index\n",
    "    df.drop(idx, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _has_title(df):\n",
    "    df.dropna(subset=['title'], inplace=True)\n",
    "    return None\n",
    "\n",
    "def _combine_dates(df):\n",
    "    df['posted_date'].fillna(df['date_added'], inplace=True)\n",
    "    df.drop('date_added', axis=1, inplace=True)\n",
    "    df.rename(columns={'posted_date': 'date'}, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _has_dates(df, columns=date_cols):\n",
    "    df.dropna(subset=columns, how='all', inplace=True)\n",
    "    return None\n",
    "\n",
    "def _date_parser(s):\n",
    "    output = pd.to_datetime(s, format='%Y-%m-%d', errors='coerce')\n",
    "    return output\n",
    "\n",
    "def _clean_and_save_chunk(file, num=0, **kwargs):\n",
    "    for chunk in pd.read_csv(file, **kwargs):\n",
    "        _has_title(chunk)\n",
    "        _has_dates(chunk, columns=date_cols)\n",
    "        _abrv_states(chunk)\n",
    "        _in_usa(chunk)\n",
    "        chunk.reset_index(drop=True).to_feather('raw_cache/data_{}.feather'.format(num))\n",
    "        num += 1\n",
    "    return num\n",
    "\n",
    "def cache_files(files, num=0, **kwargs):\n",
    "    for file in tqdm(files, desc='zip files'):\n",
    "        num = _clean_and_save_chunk(file, num=num, **kwargs)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac11149672e420aac28d52555acb444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='zip files', max=7, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder = 'raw_zips'\n",
    "files = [os.path.join(folder, file) for file in os.listdir(folder)]\n",
    "cache_files(files, usecols=zip_columns, chunksize=1e7, compression='infer', dtype=str, parse_dates=date_cols, date_parser=_date_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group data\n",
    "\n",
    "Group by day, week, and states over time\n",
    "\n",
    "## Load Feathers\n",
    "\n",
    "For now, we will only focus on the first 10 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02179d35f79c4bdca01a6789a04b5601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='load files', max=10, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>locality</th>\n",
       "      <th>region</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28999478</td>\n",
       "      <td>28972617</td>\n",
       "      <td>28999478</td>\n",
       "      <td>28999478</td>\n",
       "      <td>28999478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>349502</td>\n",
       "      <td>10094</td>\n",
       "      <td>52</td>\n",
       "      <td>1073</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sales Associate</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>CA</td>\n",
       "      <td>2016-11-14 00:00:00</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>349819</td>\n",
       "      <td>757154</td>\n",
       "      <td>4155012</td>\n",
       "      <td>745974</td>\n",
       "      <td>5460305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-08-17 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-23 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title  locality    region          posted_date day_of_week\n",
       "count          28999478  28972617  28999478             28999478    28999478\n",
       "unique           349502     10094        52                 1073           7\n",
       "top     Sales Associate   Seattle        CA  2016-11-14 00:00:00      Friday\n",
       "freq             349819    757154   4155012               745974     5460305\n",
       "first               NaN       NaN       NaN  2012-08-17 00:00:00         NaN\n",
       "last                NaN       NaN       NaN  2019-01-23 00:00:00         NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files = [file for file in os.listdir('raw_cache') if file.endswith('feather')]\n",
    "files = ['raw_cache/data_{}.feather'.format(i) for i in range(10)]\n",
    "dfs = []\n",
    "for file in tqdm(files, desc='load files'):\n",
    "    temp = pd.read_feather(file, columns=['title', 'locality', 'region', 'posted_date'])\n",
    "    dfs.append(temp.dropna(subset=['posted_date']))\n",
    "    df = pd.concat(dfs)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['day_of_week'] = pd.Categorical(df['posted_date'].dt.day_name(), categories= ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday'], ordered=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of jobs posted per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_day = df.set_index('posted_date').groupby(pd.Grouper(freq='D'))['title'].count()\n",
    "per_day.name = 'posts'\n",
    "per_day = per_day.to_frame()\n",
    "per_day.reset_index(inplace=True)\n",
    "per_day.to_feather('grouped/posts_per_day.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = df.groupby('day_of_week').count()['title']\n",
    "week.name = 'posts'\n",
    "total = week.sum()\n",
    "week = week.to_frame()\n",
    "week['ratio'] = week['posts']/total\n",
    "week.sort_values('day_of_week', inplace=True)\n",
    "week.reset_index(inplace=True)\n",
    "week.to_feather('grouped/weekly_distribution.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs per city\n",
    "\n",
    "Job posts per state also included in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_city_date = df.groupby(['locality', 'region', 'posted_date']).count()['title']\n",
    "jobs_city_date.name = 'posts'\n",
    "jobs_city_date = jobs_city_date.to_frame().reset_index()\n",
    "jobs_city_date.to_feather('grouped/jobs_city_date.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "I want to apply SVD onto a sparse matrix of counted terms from `CountVectorizer` to get the principle axes, but `TruncatedSVD` from `sklearn` is too memory intensive and takes a long time. A solution is to possibly find an on-line algorithm that is something like gradient descent for SVD. I found a couple of resources to go through for this:\n",
    "\n",
    "- [stack exchange](https://stats.stackexchange.com/questions/177007/updating-svd-decomposition-after-adding-one-new-row-to-the-matrix)\n",
    "- [gensim](https://pypi.org/project/gensim/)\n",
    "- [surprise](http://surpriselib.com/)\n",
    "- [sparsesvd](https://pypi.org/project/sparsesvd/)\n",
    "\n",
    "I'm leaning towards `gensim` at the moment, there are good resources for it and it seems like it is widely used for this expressed purpose, in particular, the `Latent Semantic Indexing` transformation. [Data Camp](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python) has a resource outlining this exact procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _process(string):\n",
    "        output = re.sub(r\"\"\"[\\d+|-|-|/]\"\"\", ' ', string.lower())\n",
    "        output = ' '.join(output.split())\n",
    "        return output\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        output = X.apply(self._process)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X will come in as a Series object.  Return a list of dictionaries corresponding to those inner lists.\n",
    "        output = []\n",
    "        for string in X:\n",
    "            d = {}\n",
    "            for v in self.vocab:\n",
    "                num = len(re.findall(r'\\b{}\\b'.format(v), string))\n",
    "                if num:\n",
    "                    d[v] = num\n",
    "            output.append(d)\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and count transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "autobot_pipeline = Pipeline([('clean', CleanTransformer()), ('count', CountVectorizer(max_features=500, stop_words='english'))])\n",
    "counts = autobot_pipeline.fit_transform(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transforms/counts_sample.pkd', 'wb') as file:\n",
    "    dill.dump(counts, file)\n",
    "with open('transforms/autobot.pkd', 'wb') as file:\n",
    "    dill.dump(autobot_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transforms/counts_sample.pkd', 'rb') as file:\n",
    "    counts = dill.load(file)\n",
    "with open('transforms/autobot.pkd', 'rb') as file:\n",
    "    autobot_pipeline = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account',\n",
       " 'accountant',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'acquisition',\n",
       " 'admin',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'advanced',\n",
       " 'advisor',\n",
       " 'affairs',\n",
       " 'agent',\n",
       " 'aide',\n",
       " 'alexa',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analytics',\n",
       " 'angeles',\n",
       " 'apparel',\n",
       " 'appliance',\n",
       " 'appliances',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'architect',\n",
       " 'area',\n",
       " 'assembler',\n",
       " 'asset',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'associates',\n",
       " 'assurance',\n",
       " 'atlanta',\n",
       " 'attendant',\n",
       " 'audit',\n",
       " 'auditor',\n",
       " 'austin',\n",
       " 'auto',\n",
       " 'automation',\n",
       " 'automotive',\n",
       " 'avp',\n",
       " 'aws',\n",
       " 'az',\n",
       " 'backroom',\n",
       " 'backup',\n",
       " 'bakery',\n",
       " 'bank',\n",
       " 'banker',\n",
       " 'banking',\n",
       " 'barista',\n",
       " 'bartender',\n",
       " 'based',\n",
       " 'bath',\n",
       " 'bbw',\n",
       " 'beach',\n",
       " 'benefits',\n",
       " 'big',\n",
       " 'bilingual',\n",
       " 'bonus',\n",
       " 'boston',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'bulk',\n",
       " 'business',\n",
       " 'buyer',\n",
       " 'ca',\n",
       " 'car',\n",
       " 'care',\n",
       " 'case',\n",
       " 'cashier',\n",
       " 'cashiers',\n",
       " 'catering',\n",
       " 'cdl',\n",
       " 'center',\n",
       " 'central',\n",
       " 'certified',\n",
       " 'chain',\n",
       " 'charlotte',\n",
       " 'chi',\n",
       " 'chicago',\n",
       " 'chili',\n",
       " 'city',\n",
       " 'claims',\n",
       " 'class',\n",
       " 'cleaner',\n",
       " 'clearance',\n",
       " 'clerk',\n",
       " 'client',\n",
       " 'clinical',\n",
       " 'cloud',\n",
       " 'cna',\n",
       " 'college',\n",
       " 'commercial',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'compliance',\n",
       " 'construction',\n",
       " 'consultant',\n",
       " 'consultative',\n",
       " 'consumer',\n",
       " 'content',\n",
       " 'contract',\n",
       " 'control',\n",
       " 'controls',\n",
       " 'cook',\n",
       " 'coordinator',\n",
       " 'corporate',\n",
       " 'cosmetics',\n",
       " 'counsel',\n",
       " 'counter',\n",
       " 'county',\n",
       " 'courtesy',\n",
       " 'creative',\n",
       " 'credit',\n",
       " 'creek',\n",
       " 'custodial',\n",
       " 'customer',\n",
       " 'cyber',\n",
       " 'cybersecurity',\n",
       " 'daily',\n",
       " 'dallas',\n",
       " 'data',\n",
       " 'database',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'deli',\n",
       " 'deliveries',\n",
       " 'delivery',\n",
       " 'department',\n",
       " 'design',\n",
       " 'designer',\n",
       " 'desk',\n",
       " 'dev',\n",
       " 'developer',\n",
       " 'development',\n",
       " 'devops',\n",
       " 'diem',\n",
       " 'diesel',\n",
       " 'digital',\n",
       " 'director',\n",
       " 'dishwasher',\n",
       " 'distribution',\n",
       " 'district',\n",
       " 'division',\n",
       " 'dot',\n",
       " 'driver',\n",
       " 'east',\n",
       " 'electrical',\n",
       " 'employee',\n",
       " 'end',\n",
       " 'engineer',\n",
       " 'engineering',\n",
       " 'enterprise',\n",
       " 'entry',\n",
       " 'environmental',\n",
       " 'equipment',\n",
       " 'escrow',\n",
       " 'estate',\n",
       " 'event',\n",
       " 'executive',\n",
       " 'experience',\n",
       " 'expert',\n",
       " 'express',\n",
       " 'facilities',\n",
       " 'facility',\n",
       " 'fc',\n",
       " 'federal',\n",
       " 'field',\n",
       " 'finance',\n",
       " 'financial',\n",
       " 'fl',\n",
       " 'floor',\n",
       " 'food',\n",
       " 'fort',\n",
       " 'francisco',\n",
       " 'freight',\n",
       " 'ft',\n",
       " 'ga',\n",
       " 'general',\n",
       " 'generalist',\n",
       " 'global',\n",
       " 'grill',\n",
       " 'grocery',\n",
       " 'group',\n",
       " 'half',\n",
       " 'handler',\n",
       " 'hba',\n",
       " 'head',\n",
       " 'health',\n",
       " 'healthcare',\n",
       " 'hills',\n",
       " 'hiring',\n",
       " 'holder',\n",
       " 'home',\n",
       " 'hospital',\n",
       " 'host',\n",
       " 'hourly',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'housekeeper',\n",
       " 'houston',\n",
       " 'hr',\n",
       " 'human',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'il',\n",
       " 'improvement',\n",
       " 'industrial',\n",
       " 'information',\n",
       " 'infrastructure',\n",
       " 'inside',\n",
       " 'inspector',\n",
       " 'installer',\n",
       " 'insurance',\n",
       " 'integration',\n",
       " 'intelligence',\n",
       " 'intern',\n",
       " 'internal',\n",
       " 'internship',\n",
       " 'inventory',\n",
       " 'investment',\n",
       " 'iv',\n",
       " 'java',\n",
       " 'job',\n",
       " 'junior',\n",
       " 'key',\n",
       " 'kitchen',\n",
       " 'la',\n",
       " 'lab',\n",
       " 'lake',\n",
       " 'las',\n",
       " 'lead',\n",
       " 'leader',\n",
       " 'learning',\n",
       " 'legal',\n",
       " 'lending',\n",
       " 'level',\n",
       " 'licensed',\n",
       " 'line',\n",
       " 'loader',\n",
       " 'loan',\n",
       " 'logistics',\n",
       " 'los',\n",
       " 'loss',\n",
       " 'lot',\n",
       " 'lpn',\n",
       " 'ltach',\n",
       " 'ma',\n",
       " 'machine',\n",
       " 'maintenance',\n",
       " 'mall',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'manufacturing',\n",
       " 'market',\n",
       " 'marketing',\n",
       " 'master',\n",
       " 'material',\n",
       " 'maternity',\n",
       " 'md',\n",
       " 'meat',\n",
       " 'mechanic',\n",
       " 'mechanical',\n",
       " 'med',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'member',\n",
       " 'mental',\n",
       " 'merchandiser',\n",
       " 'merchandising',\n",
       " 'mgmt',\n",
       " 'mgr',\n",
       " 'mi',\n",
       " 'miami',\n",
       " 'mid',\n",
       " 'mn',\n",
       " 'mo',\n",
       " 'mobile',\n",
       " 'mortgage',\n",
       " 'motherhood',\n",
       " 'multi',\n",
       " 'national',\n",
       " 'nc',\n",
       " 'nd',\n",
       " 'ne',\n",
       " 'network',\n",
       " 'new',\n",
       " 'news',\n",
       " 'night',\n",
       " 'nights',\n",
       " 'nj',\n",
       " 'non',\n",
       " 'north',\n",
       " 'nurse',\n",
       " 'nursing',\n",
       " 'ny',\n",
       " 'office',\n",
       " 'officer',\n",
       " 'oh',\n",
       " 'op',\n",
       " 'operations',\n",
       " 'operator',\n",
       " 'ops',\n",
       " 'order',\n",
       " 'originator',\n",
       " 'outback',\n",
       " 'outside',\n",
       " 'pa',\n",
       " 'park',\n",
       " 'partner',\n",
       " 'parts',\n",
       " 'patient',\n",
       " 'payroll',\n",
       " 'performance',\n",
       " 'personal',\n",
       " 'pest',\n",
       " 'pharmacy',\n",
       " 'phlebotomist',\n",
       " 'phoenix',\n",
       " 'physical',\n",
       " 'planner',\n",
       " 'planning',\n",
       " 'plant',\n",
       " 'platform',\n",
       " 'plaza',\n",
       " 'plus',\n",
       " 'pm',\n",
       " 'portfolio',\n",
       " 'preferred',\n",
       " 'prep',\n",
       " 'president',\n",
       " 'prevention',\n",
       " 'pricing',\n",
       " 'principal',\n",
       " 'print',\n",
       " 'private',\n",
       " 'prn',\n",
       " 'pro',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'processor',\n",
       " 'procurement',\n",
       " 'produce',\n",
       " 'producer',\n",
       " 'product',\n",
       " 'production',\n",
       " 'products',\n",
       " 'professional',\n",
       " 'program',\n",
       " 'programmer',\n",
       " 'programs',\n",
       " 'project',\n",
       " 'protection',\n",
       " 'ps',\n",
       " 'pt',\n",
       " 'public',\n",
       " 'qa',\n",
       " 'quality',\n",
       " 'rd',\n",
       " 'real',\n",
       " 'receiving',\n",
       " 'recruiter',\n",
       " 'recruiting',\n",
       " 'refrigeration',\n",
       " 'region',\n",
       " 'regional',\n",
       " 'registered',\n",
       " 'regulatory',\n",
       " 'rehab',\n",
       " 'rehabilitation',\n",
       " 'relations',\n",
       " 'relationship',\n",
       " 'reliability',\n",
       " 'rep',\n",
       " 'repair',\n",
       " 'reporting',\n",
       " 'representative',\n",
       " 'req',\n",
       " 'required',\n",
       " 'research',\n",
       " 'resources',\n",
       " 'restaurant',\n",
       " 'retail',\n",
       " 'risk',\n",
       " 'rn',\n",
       " 'robert',\n",
       " 'route',\n",
       " 'safety',\n",
       " 'sales',\n",
       " 'san',\n",
       " 'santa',\n",
       " 'sap',\n",
       " 'satellite',\n",
       " 'sc',\n",
       " 'science',\n",
       " 'scientist',\n",
       " 'seafood',\n",
       " 'seasonal',\n",
       " 'seattle',\n",
       " 'secret',\n",
       " 'security',\n",
       " 'senior',\n",
       " 'server',\n",
       " 'service',\n",
       " 'services',\n",
       " 'shift',\n",
       " 'shop',\n",
       " 'shopping',\n",
       " 'sign',\n",
       " 'site',\n",
       " 'small',\n",
       " 'social',\n",
       " 'softlines',\n",
       " 'software',\n",
       " 'solution',\n",
       " 'solutions',\n",
       " 'sonoma',\n",
       " 'south',\n",
       " 'spanish',\n",
       " 'specialist',\n",
       " 'spectrum',\n",
       " 'springs',\n",
       " 'square',\n",
       " 'sr',\n",
       " 'st',\n",
       " 'stack',\n",
       " 'staff',\n",
       " 'staffing',\n",
       " 'state',\n",
       " 'states',\n",
       " 'steakhouse',\n",
       " 'stock',\n",
       " 'store',\n",
       " 'strategic',\n",
       " 'strategy',\n",
       " 'student',\n",
       " 'success',\n",
       " 'summer',\n",
       " 'supervisor',\n",
       " 'supply',\n",
       " 'support',\n",
       " 'systems',\n",
       " 'talent',\n",
       " 'tax',\n",
       " 'team',\n",
       " 'teammate',\n",
       " 'tech',\n",
       " 'technical',\n",
       " 'technician',\n",
       " 'technologist',\n",
       " 'technology',\n",
       " 'teller',\n",
       " 'territory',\n",
       " 'test',\n",
       " 'text',\n",
       " 'th',\n",
       " 'therapist',\n",
       " 'time',\n",
       " 'tn',\n",
       " 'tool',\n",
       " 'town',\n",
       " 'trainee',\n",
       " 'training',\n",
       " 'transportation',\n",
       " 'transporter',\n",
       " 'treasury',\n",
       " 'truck',\n",
       " 'tv',\n",
       " 'tx',\n",
       " 'unit',\n",
       " 'united',\n",
       " 'univ',\n",
       " 'university',\n",
       " 'utility',\n",
       " 'ux',\n",
       " 'va',\n",
       " 'valley',\n",
       " 'vegas',\n",
       " 'vice',\n",
       " 'video',\n",
       " 'village',\n",
       " 'vitamin',\n",
       " 'vp',\n",
       " 'wa',\n",
       " 'warehouse',\n",
       " 'washington',\n",
       " 'wealth',\n",
       " 'web',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'west',\n",
       " 'williams',\n",
       " 'work',\n",
       " 'worker',\n",
       " 'workweek',\n",
       " 'writer',\n",
       " 'york']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autobot_pipeline.named_steps['count'].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build job vocabulary\n",
    "\n",
    "Use the US Bureau of Labor and Statistics jobs descriptions to build a vocabulary to search the job post data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('other_data/BLS_job_descriptions.csv', usecols=['occ_title'])\n",
    "test.drop_duplicates(inplace=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "idxs = [test.index[0], test.index[-1]]\n",
    "test.drop(idxs, inplace=True)\n",
    "test = test['occ_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test.str.replace(r'\\W', ' ')\n",
    "temp = temp.str.split(expand=True).stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Dimensionality\n",
    "\n",
    "I want to apply SVD onto the count sparse matrix to get the principle axes, but `TruncatedSVD` from `sklearn` is too memory intensive and takes a long time. A solution is to possibly find an on-line algorithm that is something like gradient descent for SVD. I found a couple of resources to go through for this:\n",
    "\n",
    "- [stack exchange](https://stats.stackexchange.com/questions/177007/updating-svd-decomposition-after-adding-one-new-row-to-the-matrix)\n",
    "- [gensim](https://pypi.org/project/gensim/)\n",
    "- [surprise](http://surpriselib.com/)\n",
    "- [sparsesvd](https://pypi.org/project/sparsesvd/)\n",
    "\n",
    "I'm leaning towards `gensim` at the moment, there are good resources for it and it seems like it is widely used for this expressed purpose, in particular, the `Latent Semantic Indexing` transformation. [Data Camp](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python) has a resource outlining this exact procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50)\n",
    "svd.fit(counts, df['region'])\n",
    "np.shape(svd.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transforms/svd_sample.pkd', 'wb') as file:\n",
    "    dill.dump(svd, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svd_sample.pkd', 'rb') as file:\n",
    "    svd = dill.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
