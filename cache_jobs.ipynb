{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "import dill\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn import base\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and cache .zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_feather('census.feather')\n",
    "states = pd.read_feather('us_states.feather')\n",
    "states_dict = states.set_index('STATE').to_dict()['Abrv']\n",
    "\n",
    "zip_columns = ['title', 'brand', 'category', 'locality', 'region', 'date_added', 'posted_date']\n",
    "\n",
    "def _no_location(df):\n",
    "    truth = np.array(df['region'].isna().tolist() and df['locality'].isna().tolist())\n",
    "    idx = df[truth].index\n",
    "    df.drop(idx, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _abrv_states(df):\n",
    "    df['region'] = df['region'].str.upper().replace(states_dict)\n",
    "    return None\n",
    "\n",
    "def _in_usa(df):\n",
    "    truth = df[['region']].isin(states_dict.values())['region']\n",
    "    idx = df[~truth].index\n",
    "    df.drop(idx, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _has_title(df):\n",
    "    df.dropna(subset=['title'], inplace=True)\n",
    "    return None\n",
    "\n",
    "def _combine_dates(df):\n",
    "    df['posted_date'].fillna(df['date_added'], inplace=True)\n",
    "    df.drop('date_added', axis=1, inplace=True)\n",
    "    df.rename(columns={'posted_date': 'date'}, inplace=True)\n",
    "    return None\n",
    "\n",
    "def _parse_date(df, columns=['date']):\n",
    "    for column in columns:\n",
    "        df[column] = pd.to_datetime(df[column], yearfirst=True)\n",
    "    return None\n",
    "\n",
    "def _clean_and_save_chunk(file, columns, num=0, chunksize=1e7, compression='infer'):\n",
    "    for chunk in pd.read_csv(file, usecols=columns, chunksize=chunksize, compression=compression):\n",
    "        _has_title(chunk)\n",
    "        _abrv_states(chunk)\n",
    "        _in_usa(chunk)\n",
    "        chunk.reset_index(drop=True).to_feather('raw_cache/data_%s.feather' %num)\n",
    "        num += 1\n",
    "    return num\n",
    "\n",
    "def cache_files(files, columns, num=0, chunksize=1e7, compression='infer'):\n",
    "    for file in tqdm(files, desc='zip files'):\n",
    "        num = _clean_and_save_chunk(file, columns, num, chunksize, compression)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['raw_zips/jobs_{}.zip'.format(i) for i in range(1,8)]\n",
    "cache_files(files, zip_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group data\n",
    "\n",
    "Group by day, week, and states over time\n",
    "\n",
    "## Load Feathers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [file for file in os.listdir('raw_cache') if file.endswith('feather')]\n",
    "files = ['raw_cache/data_{}.feather'.format(i) for i in range(10)]\n",
    "dfs = []\n",
    "for file in tqdm(files, desc='load files'):\n",
    "    temp = pd.read_feather(file, columns=['title', 'region', 'posted_date'])\n",
    "    dfs.append(temp.dropna(subset=['posted_date']))\n",
    "    df = pd.concat(dfs)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['posted_date'] = pd.to_datetime(df['posted_date'], yearfirst=True, errors='coerce')\n",
    "df['day_of_week'] = df['posted_date'].dt.day_name()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of jobs posted per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_day = df.set_index('posted_date').groupby(pd.Grouper(freq='D'))['title'].count()\n",
    "per_day.name = 'posts'\n",
    "per_day = per_day.to_frame()\n",
    "per_day.reset_index(inplace=True)\n",
    "per_day.to_feather('grouped/posts_per_day.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = df.groupby('day_of_week').count()['title']\n",
    "week.name = 'posts'\n",
    "total = week.sum()\n",
    "week = week.to_frame()\n",
    "week.reset_index(inplace=True)\n",
    "\n",
    "week_map = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}\n",
    "week['order'] = week['day_of_week'].replace(week_map)\n",
    "week.sort_values('order', inplace=True)\n",
    "week.reset_index(inplace=True)\n",
    "week.to_feather('grouped/weekly_distribution.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Taking the titles from the job posts, I'll want to eliminate some of the words or symbols with `CountVectorizer`, which will output a sparse matrix consisting of the input string and respective counts. The number of unique, interesting keywords will be very large, to make more sense of the data, I will want to perform dimensionality reduction via `TruncatedSVD` with the `CountVectorizer` sparse matrix output. The principle components will generally be the sectors of industry.\n",
    "\n",
    "At this point, a good stop gap is to visualize the sectors of industry, maybe even as a function of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _process(string):\n",
    "        output = re.sub(r\"\"\"[\\d+|-|-|/]\"\"\", ' ', string.lower())\n",
    "        output = ' '.join(output.split())\n",
    "        return output\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        output = X.apply(self._process)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X will come in as a Series object.  Return a list of dictionaries corresponding to those inner lists.\n",
    "        output = []\n",
    "        for string in X:\n",
    "            d = {}\n",
    "            for v in self.vocab:\n",
    "                num = len(re.findall(r'\\b{}\\b'.format(v), string))\n",
    "                if num:\n",
    "                    d[v] = num\n",
    "            output.append(d)\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and count transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autobot_pipeline = Pipeline([('clean', CleanTransformer()), ('count', CountVectorizer(max_features=500, stop_words='english'))])\n",
    "counts = autobot_pipeline.fit_transform(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transforms/counts_sample.pkd', 'wb') as file:\n",
    "    dill.dump(counts, file)\n",
    "with open('transforms/autobot.pkd', 'wb') as file:\n",
    "    dill.dump(autobot_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transforms/counts_sample.pkd', 'rb') as file:\n",
    "    counts = dill.load(file)\n",
    "with open('transforms/autobot.pkd', 'rb') as file:\n",
    "    autobot_pipeline = dill.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50)\n",
    "svd.fit(counts, df['region'])\n",
    "np.shape(svd.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transforms/svd_sample.pkd', 'wb') as file:\n",
    "    dill.dump(svd, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('svd_sample.pkd', 'rb') as file:\n",
    "    svd = dill.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
