{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas(desc='apply')\n",
    "from scipy import sparse\n",
    "import dill\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn import base\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_df(file, **kwargs):\n",
    "    df = pd.read_feather(file, **kwargs)\n",
    "    _within_range(df)\n",
    "    df.dropna(subset=['posted_date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def _within_range(df):\n",
    "    start = pd.datetime(2017, 1, 1)\n",
    "    end = pd.datetime(2018, 7, 1)\n",
    "    truth = ~df['posted_date'].isin(pd.date_range(start, end))\n",
    "    df.drop(df[truth].index, inplace=True)\n",
    "\n",
    "class TextPreProcess(base.BaseEstimator, base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    Input  : document list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ignore):\n",
    "        self.en_stop = set(stopwords.words('english')) # English stop words list\n",
    "        self.tokenizer = RegexpTokenizer(r'[a-z]+&?[a-z]+')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.replace = ignore\n",
    "    \n",
    "    def _process(self, text):\n",
    "        raw = text.lower()\n",
    "        for key, val in self.replace.items():\n",
    "            raw = re.sub(key, val, raw)\n",
    "        tokens = self.tokenizer.tokenize(raw)\n",
    "        stopped_tokens = [i for i in tokens if not i in self.en_stop]\n",
    "        lemma_tokens = [self.lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "        output = ' '.join(lemma_tokens)\n",
    "        return output\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        output = X.apply(self._process)\n",
    "        return output\n",
    "    \n",
    "def clean_titles(files, ignore_dict, columns=['title', 'region', 'posted_date']):\n",
    "    tpp = TextPreProcess(ignore_dict)\n",
    "\n",
    "    i = 0\n",
    "    for file in tqdm(files, desc='clean titles'):\n",
    "        df = _get_df(file, columns=columns)\n",
    "        df['title'] = tpp.fit_transform(df['title'])\n",
    "        df.reset_index(drop=True).to_feather('cleaned_cache/data_{}.feather'.format(i))\n",
    "        i += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d05a25dcf64c149bc0c197188d3d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='clean titles', max=41, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folder = 'raw_cache'\n",
    "files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.feather')]\n",
    "\n",
    "ignore = pd.read_feather('other_data/ignore.feather')\n",
    "ignore_dict = ignore.set_index('regex').to_dict()['sub']\n",
    "\n",
    "clean_titles(files, ignore_dict=ignore_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(files, fraction=20):\n",
    "    dfs = []\n",
    "    for file in tqdm(files, desc='feathers'):\n",
    "        df = pd.read_feather(file)\n",
    "        dfs.append(df.sample(df.shape[0]//fraction))\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e775c4f543e542f39d0c0e81da099142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='feathers', max=41, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 26.9 s, sys: 13.2 s, total: 40.2 s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folder = 'cleaned_cache'\n",
    "files = [os.path.join(folder, 'data_{}.feather'.format(i)) for i in range(len(os.listdir(folder)))]\n",
    "\n",
    "df = downsample(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
